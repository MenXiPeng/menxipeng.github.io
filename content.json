{"meta":{"title":"门✈❣Bツ⊙g❦✈♂","subtitle":"","description":"","author":"John Doe","url":"http://yoursite.com","root":"/"},"pages":[{"title":"所有分类","date":"2020-07-30T02:25:43.528Z","updated":"2020-07-30T02:25:43.528Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"我的朋友们","date":"2020-07-30T02:50:53.214Z","updated":"2020-07-30T02:50:53.214Z","comments":true,"path":"friends/index.html","permalink":"http://yoursite.com/friends/index.html","excerpt":"","text":""},{"title":"关于","date":"2020-07-30T04:15:46.552Z","updated":"2020-07-30T04:15:46.552Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"kkkkkk"},{"title":"","date":"2020-07-30T02:45:46.337Z","updated":"2020-07-30T02:45:46.337Z","comments":true,"path":"messages/index.html","permalink":"http://yoursite.com/messages/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2020-07-30T02:26:56.693Z","updated":"2020-07-30T02:26:56.693Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Hadoop 3.x 单机安装","slug":"Hadoop","date":"2020-07-29T16:00:00.000Z","updated":"2020-07-30T10:23:45.055Z","comments":true,"path":"2020/07/30/Hadoop/","link":"","permalink":"http://yoursite.com/2020/07/30/Hadoop/","excerpt":"关于hadoop 3.x 版本基础安装个人总结，建议以官网为指导进行配置，不同版本不一致，本教程适用于3.1.x官网地址：https://hadoop.apache.org/docs/current/","text":"关于hadoop 3.x 版本基础安装个人总结，建议以官网为指导进行配置，不同版本不一致，本教程适用于3.1.x官网地址：https://hadoop.apache.org/docs/current/ 一、下载hadoophttps://hadoop.apache.org/releases.html 二、解压hadoop包tar -zxvf hadoop-3.1.2.tar.gz 三、修改hosts文件1. vim /etc/hosts 2. 添加 0.0.0.0(内网ip) master master 3. source /etc/profile 四、生成ssh密钥1. ssh-keygen -t rsa -P &#39;&#39; # 第一个回车-输入要生成的文件名(默认id_rsa) # 第二个回车-输入生成密钥密码(默认为空) # 第三个回车-重复输入密码(默认为空) 2. cat /root/.ssh/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys #master主机(互信密钥) # 不设置密钥互信会导致启动时出现权限不足的异常 五、配置hadoop Home1. vim /etc/profile 2. 添加 #hadoop enviroment hadoop环境变量 export HADOOP_HOME=/usr/local/hadoop-3.1.2/ #hadoop 根目录 export PATH=&quot;$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH&quot; export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop 3. source /etc/profile #重新加载源文件 六、修改hadoop 配置文件 $HADOOP_HOME # hadoop 根目录1234567891011121314151617$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;hadoop-env.sh # 修改行 export JAVA_HOME&#x3D;&#x2F;usr&#x2F;java&#x2F;jdk1.8.0_112&#x2F;$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;slaves (添加集群节点hostname,单机节点不需要修改)$HADOOP_HOME&#x2F;etc&#x2F;hadoop&#x2F;core-site.xml #添加如下内容：&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt; &lt;value&gt;hdfs:&#x2F;&#x2F;master:9000&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;&#x2F;name&gt; &lt;value&gt;131072&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt; &lt;value&gt;&#x2F;usr&#x2F;local&#x2F;hadoop-3.1.2&#x2F;tmp&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; $HADOOP_HOME/etc/hadoop/hdfs-site.xml #添加如下内容12345678910111213141516171819202122232425262728293031&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt; &lt;value&gt;master:50090&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;&#x2F;name&gt; &lt;value&gt;1&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt; &lt;value&gt;file:&#x2F;usr&#x2F;local&#x2F;hadoop-3.1.2&#x2F;hdfs&#x2F;name&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt; &lt;value&gt;file:&#x2F;usr&#x2F;local&#x2F;hadoop-3.1.2&#x2F;hdfs&#x2F;data&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;&#x2F;name&gt; &lt;value&gt;false&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.enable&lt;&#x2F;name&gt; &lt;value&gt;true&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;dfs.client.block.write.replace-datanode-on-failure.policy&lt;&#x2F;name&gt; &lt;value&gt;NEVER&lt;&#x2F;value&gt; &lt;&#x2F;property&gt;&lt;&#x2F;configuration&gt; $HADOOP_HOME/etc/hadoop/mapred-site.xml(cp mapred-site.xml.template mapred-site.xml)123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;&#x2F;name&gt; &lt;value&gt;yarn&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt; &lt;value&gt;master:10020&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt; &lt;value&gt;master:19888&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;mapreduce.application.classpath&lt;&#x2F;name&gt; &lt;value&gt;$HADOOP_MAPRED_HOME&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;*:$HADOOP_MAPRED_HOME&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;lib&#x2F;*&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; --&gt;&lt;&#x2F;configuration&gt; $HADOOP_HOME/etc/hadoop/yarn-site.xml12345678910111213141516171819202122232425262728293031&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt; &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;&#x2F;name&gt; &lt;value&gt;master:8032&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;&#x2F;name&gt; &lt;value&gt;master:8030&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;&#x2F;name&gt; &lt;value&gt;master:8031&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;&#x2F;name&gt; &lt;value&gt;master:8033&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;&#x2F;name&gt; &lt;value&gt;master:8088&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; &lt;!-- &lt;property&gt; &lt;name&gt;yarn.nodemanager.env-whitelist&lt;&#x2F;name&gt; &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME&lt;&#x2F;value&gt; &lt;&#x2F;property&gt; --&gt;&lt;&#x2F;configuration&gt; 七、格式化namenode 节点 hadoop namenode -format 八、脚本配置1.启动 #!/bin/bash echo -e &quot;\\033[31m ========Start The Cluster======== \\033[0m&quot; echo -e &quot;\\033[31m Starting Hadoop Now !!! \\033[0m&quot; /opt/hadoop-2.7.3/sbin/start-all.sh echo -e &quot;\\033[31m Starting Spark Now !!! \\033[0m&quot; /opt/spark-2.1.0-bin-hadoop2.7/sbin/start-all.sh echo -e &quot;\\033[31m The Result Of The Command \\&quot;jps\\&quot; : \\033[0m&quot; jps echo -e &quot;\\033[31m ========END======== \\033[0m&quot; 2.停止脚本 #!/bin/bash echo -e &quot;\\033[31m ===== Stoping The Cluster ====== \\033[0m&quot; echo -e &quot;\\033[31m Stoping Spark Now !!! \\033[0m&quot; /opt/spark-2.1.0-bin-hadoop2.7/sbin/stop-all.sh echo -e &quot;\\033[31m Stopting Hadoop Now !!! \\033[0m&quot; /opt/hadoop-2.7.3/sbin/stop-all.sh echo -e &quot;\\033[31m The Result Of The Command \\&quot;jps\\&quot; : \\033[0m&quot; jps echo -e &quot;\\033[31m ======END======== \\033[0m&quot; 九、查看节点 jps----- Jps ----- DataNode ----- NameNode ----- NodeManager ----- SecondaryNameNode 看到所有节点代表启动成功 十、root启动报错start-dfs.sh / stop-dfs.sh HDFS_DATANODE_USER=root HADOOP_SECURE_DN_USER=hdfs HDFS_NAMENODE_USER=root HDFS_SECONDARYNAMENODE_USER=root start-yarn.sh / stop-yarn.sh YARN_RESOURCEMANAGER_USER=root HADOOP_SECURE_DN_USER=yarn YARN_NODEMANAGER_USER=root 参考 http://www.cnblogs.com/purstar/p/6293605.html","categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hadoop搭建","slug":"大数据/Hadoop搭建","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"欢迎光临红浪漫","slug":"index","date":"2020-07-29T11:52:34.541Z","updated":"2020-07-30T08:49:23.006Z","comments":true,"path":"2020/07/29/index/","link":"","permalink":"http://yoursite.com/2020/07/29/index/","excerpt":"","text":"男宾三位里边请～","categories":[],"tags":[]}],"categories":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"},{"name":"Hadoop搭建","slug":"大数据/Hadoop搭建","permalink":"http://yoursite.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/Hadoop%E6%90%AD%E5%BB%BA/"}],"tags":[{"name":"大数据","slug":"大数据","permalink":"http://yoursite.com/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]}